{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ultimate script to prepare the raw freesurfer data**\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- just put everything into one textfile\n",
    "- check\n",
    "- add clinics\n",
    "\n",
    "*cross sectional AND longitudinal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# importing custom functions\n",
    "code_dir = ('/home/barbora/Documents/Projects/Normative_Models/ESO/code')\n",
    "os.chdir(code_dir)\n",
    "import clinics_desc_functions as custom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_dir = '/home/barbora/Documents/Projects/Normative_Models' \n",
    "cs_dir = '/home/barbora/Documents/Projects/Normative_Models/ESO/backup/fit_external_cs'\n",
    "long_dir = '/home/barbora/Documents/Projects/Normative_Models/ESO/backup/fit_external_long'\n",
    "target_dir = '/home/barbora/Documents/Projects/Normative_Models/ESO/models/sensitivity/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clinics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import clinics\n",
    "clinics = pd.read_excel(os.path.join(main_dir, 'ESO', 'clinics_parsed.xlsx'), thousands=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and adding index\n",
    "index = [i.lstrip('ESO') for i in clinics['osobní kód (Hydra ID).1']]\n",
    "clinics.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns and categorical variables\n",
    "clinics['category'] = clinics['Kategorie osoby'].str[:].str.upper().map({'PACIENT':'Patient', 'KONTROLA':'Control', 'SOUROZENEC':'Sibling', 'HIGH RISK':'High_risk'})\n",
    "clinics['inclusion'] = clinics['Zařazení osoby'].str[:].str.upper().map({'ZAŘAZENA':'Included', 'ZAŘAZENA S VÝHRADAMI':'Included with Reservations'})\n",
    "\n",
    "sex_nums = {\"Pohlaví\": {\"žena\":0, \"muž\":1}}\n",
    "clinics = clinics.replace(sex_nums)\n",
    "\n",
    "clinics.rename(columns={\"Pohlaví\":\"sex\", \n",
    "                        \"Je pravák?\":\"right_handed\", \n",
    "                        \"Věk při 1. vizitě\":\"age_1\", \n",
    "                        \"Věk při 2. vizitě\":\"age_2\", \n",
    "                        \"Věk při 3. vizitě\":\"age_3\",\n",
    "                        \"Klinika/DUP [měsíce] 1\":\"length_of_symptoms\",\n",
    "                        \"Klinika/Délka onem. do MRI 1 [měsíce] 1\":\"length_of_disease\",\n",
    "                        \"Klinika/Dg. 1\":\"diagnosis\",},\n",
    "                        inplace=True)\n",
    "\n",
    "for i in range(1,4):\n",
    "    clinics['age_'+str(i)] = clinics['age_'+str(i)]/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pick Preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "preproc = 'long' # cs or long\n",
    "modality = 'SurfArea' #'SurfArea' or 'thickness'\n",
    "\n",
    "if preproc == 'long':\n",
    "    v1 = pd.read_csv(glob(long_dir+'/*'+modality+'*_1.txt')[0], sep =';', index_col=0)\n",
    "    v2 = pd.read_csv(glob(long_dir+'/*'+modality+'*_2.txt')[0], sep =';', index_col=0)\n",
    "if preproc == 'cs':\n",
    "    v1_c = pd.read_csv(glob(cs_dir+'/*_C_'+modality+'*_1.csv')[0], sep =';', index_col=0)\n",
    "    v2_c = pd.read_csv(glob(cs_dir+'/*_C_'+modality+'*_2.csv')[0], sep =';', index_col=0)\n",
    "    v1_p = pd.read_csv(glob(cs_dir+'/*_P_'+modality+'*_1.csv')[0], sep =';', index_col=0)\n",
    "    v2_p = pd.read_csv(glob(cs_dir+'/*_P_'+modality+'*_2.csv')[0], sep =';', index_col=0)\n",
    "\n",
    "    v1 = pd.concat([v1_c, v1_p])\n",
    "    v2 = pd.concat([v2_c, v2_p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All good dataset are same sizes with same subjects\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename indicies for future merging\n",
    "index = [i.split('_')[0] for i in v1.index]\n",
    "v1.index = index\n",
    "\n",
    "index = [i.split('_')[0] for i in v2.index]\n",
    "v2.index = index\n",
    "\n",
    "# Delete siblings and high risk data\n",
    "v1.drop(v1.iloc[np.where(np.array([i[0] for i in v1.index]) == 'S')[0]].index, inplace = True)\n",
    "v2.drop(v2.iloc[np.where(np.array([i[0] for i in v2.index]) == 'S')[0]].index, inplace = True)\n",
    "\n",
    "v1.drop(v1.iloc[np.where(np.array([i[0] for i in v1.index]) == 'H')[0]].index, inplace = True)\n",
    "v2.drop(v2.iloc[np.where(np.array([i[0] for i in v2.index]) == 'H')[0]].index, inplace = True)\n",
    "\n",
    "\n",
    "# Do we need to delete some other data? \n",
    "if (v1.index.intersection(v2.index).shape[0] == v1.shape[0]) & (v1.index.intersection(v2.index).shape[0] == v2.shape[0]):\n",
    "    print('All good dataset are same sizes with same subjects')\n",
    "else: \n",
    "    print('Something is not good, some subjects might be redundant, chceck teh intersection of visits')\n",
    "\n",
    "# Sort and get ready for possible concatenation\n",
    "v1 = v1.sort_index()\n",
    "v2 = v2.sort_index()\n",
    "\n",
    "sum(v1.index == v2.index) == v1.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding the clinic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = [v1, v2]\n",
    "\n",
    "# visit 1 \n",
    "for i, visit in enumerate(visits):\n",
    "    \n",
    "    # Basics\n",
    "    visit = pd.concat([clinics[['category', 'sex', 'age_'+str(i+1), \"diagnosis\", \"length_of_symptoms\", \"length_of_disease\"]],visit], axis=1, join='inner')\n",
    "    visit['visit'] = i+1\n",
    "    visit['preproc'] = preproc\n",
    "    visit['site'] = 'NUDZ'\n",
    "    visit['sitenum'] = 1000\n",
    "    visit.rename(columns={\"age_\"+str(i+1):\"age\"}, inplace=True)\n",
    "    #visit['diagnosis'] = visit['diagnosis'].astype('category')\n",
    "    ###\n",
    "    # PANSS\n",
    "    ###\n",
    "\n",
    "    PANSS = [col for col in clinics.columns if \"PANSS \"+str(i+1) in col]\n",
    "\n",
    "    visit = pd.concat([visit, clinics[PANSS]], join=\"inner\", axis=1)\n",
    "    visit.rename(columns=lambda x: x.replace('PANSS/PANSS '+str(i+1)+' / ', 'PANSS_'), inplace=True)\n",
    "\n",
    "    # Rename the PANSS variables\n",
    "    PANSS = [col for col in visit.columns if \"PANSS_\" in col]\n",
    "\n",
    "    # We need to replace empy values (originally described as 'x') with NaN\n",
    "    mapping = {'x': np.nan}\n",
    "    visit = visit.applymap(lambda s: mapping.get(s) if s in mapping else s)\n",
    "\n",
    "    # Create indicies of the three PANSS chapters\n",
    "    PANSS_P = [col for col in visit.columns if \"PANSS_P\" in col][0:-1]\n",
    "    PANSS_N = [col for col in visit.columns if \"PANSS_N\" in col][0:-1]\n",
    "    PANSS_G = [col for col in visit.columns if \"PANSS_G\" in col][0:-1]\n",
    "\n",
    "    visit[\"PANSS_sumP\"] = visit[[col for col in visit.columns if \"PANSS_P\" in col][0:-1]].sum(axis=1)\n",
    "    visit[\"PANSS_sumN\"] = visit[[col for col in visit.columns if \"PANSS_N\" in col][0:-1]].sum(axis=1)\n",
    "    visit[\"PANSS_sumG\"] = visit[[col for col in visit.columns if \"PANSS_G\" in col][0:-1]].sum(axis=1)\n",
    "\n",
    "\n",
    "    ###\n",
    "    # GAF\n",
    "    ###\n",
    "    GAF = [col for col in clinics.columns if \"Škály/GAF \"+str(i+1) in col]\n",
    "\n",
    "    visit = pd.concat([visit, clinics[GAF]], join=\"inner\", axis=1)\n",
    "    visit.rename(columns={GAF[0]:\"GAF\"}, inplace=True)\n",
    "\n",
    "    visit['full_index']=visit.index+'_'+str(i+1)\n",
    "\n",
    "    ###\n",
    "    # REMOVING SUBJECTS BASED ON INCLUSION/EXCLUSION CRITERIA\n",
    "    ###\n",
    "    # dele subjects younger than 18\n",
    "    visit = visit.loc[visit['age']>=18]\n",
    "    # delete subjects with disease duration longer than 24 months\n",
    "    visit = visit.loc[(visit['length_of_disease']<=24) | (visit['category']=='Control')]\n",
    "    # dele subjects with diagnosis F25\n",
    "    visit = visit.loc[visit['diagnosis'].isnull() | visit['diagnosis'].str.contains('F20|F23')]\n",
    "\n",
    "    # change data in the list\n",
    "    visits[i] = visit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating one more dataset that is a m erge of both visits (if needed for plottng or something)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([visits[0], visits[1]], axis=0)\n",
    "all = all.reset_index()\n",
    "all.index = all['full_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits[0].to_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'_1.csv'), sep=' ', index=True)\n",
    "visits[1].to_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'_2.csv'), sep=' ', index=True)\n",
    "all.to_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'.csv'), sep=' ', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Euler number is not available for longitudinal data, so we are inserting the cs one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same number of subjects in long and cs dataset\n"
     ]
    }
   ],
   "source": [
    "all_long = pd.read_csv(os.path.join(target_dir, 'all_long_'+modality+'.csv'), sep=' ', index_col=0)\n",
    "all_cs = pd.read_csv(os.path.join(target_dir, 'all_cs_'+modality+'.csv'), sep=' ', index_col=0)\n",
    "\n",
    "if all_long.shape[0] == all_cs.shape[0]:\n",
    "    print('same number of subjects in long and cs dataset')\n",
    "\n",
    "all_long = pd.concat([all_long, all_cs[all_cs.columns.difference(all_long.columns)]], axis = 1, join='inner')\n",
    "all_long.shape[0] == all_cs.shape[0]\n",
    "\n",
    "all_long.to_csv(os.path.join(target_dir, 'all_long_'+modality+'.csv'), sep=' ', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same number of subjects in long and cs dataset\n",
      "same number of subjects in long and cs dataset\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "\n",
    "    all_long = pd.read_csv(os.path.join(target_dir, 'all_long_'+modality+'_'+str(i)+'.csv'), sep=' ', index_col=0)\n",
    "    all_cs = pd.read_csv(os.path.join(target_dir, 'all_cs_'+modality+'_'+str(i)+'.csv'), sep=' ', index_col=0)\n",
    "\n",
    "    if all_long.shape[0] == all_cs.shape[0]:\n",
    "        print('same number of subjects in long and cs dataset')\n",
    "\n",
    "    all_long = pd.concat([all_long, all_cs[all_cs.columns.difference(all_long.columns)]], axis = 1, join='inner')\n",
    "    all_long.shape[0] == all_cs.shape[0]\n",
    "\n",
    "    all_long.to_csv(os.path.join(target_dir, 'all_long_'+modality+'_'+str(i)+'.csv'), sep=' ', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QC**\n",
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = pd.read_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'_1.csv'), sep=' ', index_col=0)\n",
    "v2 = pd.read_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'_2.csv'), sep=' ', index_col=0)\n",
    "all = pd.read_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'.csv'), sep=' ', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.drop(columns = 'SurfaceHoles', inplace = True)\n",
    "v2.drop(columns = 'SurfaceHoles', inplace = True)\n",
    "\n",
    "v1 = custom.en_qc(v1)\n",
    "v2 = custom.en_qc(v2)\n",
    "\n",
    "v1 = v1.loc[v1.index.intersection(v2.index)]\n",
    "v2 = v2.loc[v1.index.intersection(v2.index)]\n",
    "\n",
    "all = all.iloc[np.where(all['index'].isin(list(v1.index.intersection(v2.index)))==True)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "all.to_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'_qc.csv'), sep=' ', index=True)\n",
    "v1.to_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'_1_qc.csv'), sep=' ', index=True)\n",
    "v2.to_csv(os.path.join(target_dir, 'all_'+preproc+'_'+modality+'_2_qc.csv'), sep=' ', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3951f2f4cd92dfb9d3e6d0b790d47ccf8f4798183357503ed7e741f4d780229a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PCN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
